{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"../../lib\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = '../../checkpoints/faster_rcnn_inception_resnet_v2_atrous_oid'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('../../lib/object_detection/data', 'oid_bbox_trainable_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_graph_def = tf.GraphDef()\n",
    "with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../data/dryrun3_m18/keyframe.txt', 'r') as fin:\n",
    "    test_img_path = ['/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun-updated/data/video_shot_boundaries/' + line.strip() for line in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun-updated/data/video_shot_boundaries/representative_frames/v_11KGnqXIBJqs6NeG/v_11KGnqXIBJqs6NeG_14.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_to_good_vid_id = {}\n",
    "good_to_bad_vid_id = {}\n",
    "bad_to_good_kf_id = {}\n",
    "good_to_bad_kf_id = {}\n",
    "\n",
    "with open('../../../../data/dryrun3_m18/masterShotBoundary.msb', 'r') as fin:\n",
    "    for line in fin:\n",
    "        row = line.split()\n",
    "\n",
    "        good_kf_id = row[1]\n",
    "        good_vid_id = row[1].split('_')[0]\n",
    "        bad_vid_id = row[0]\n",
    "        bad_kf_id = row[0] + '_' + good_kf_id.split('_')[-1]\n",
    "        \n",
    "        bad_to_good_vid_id[bad_vid_id] = good_vid_id\n",
    "        good_to_bad_vid_id[good_vid_id] = bad_vid_id                \n",
    "        bad_to_good_kf_id[bad_kf_id] = good_kf_id\n",
    "        good_to_bad_kf_id[good_kf_id] = bad_kf_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: v_WZXqwgba8WWTNhQz_41 has an image but not found in msb\n",
      "WARNING: v_yxdffFa9mkXG8sB0_44 has an image but not found in msb\n",
      "WARNING: v_pjPPXHzs3dXCs7gK_4 has an image but not found in msb\n",
      "WARNING: v_ZfnMpvozSpjddjsx_21 has an image but not found in msb\n",
      "WARNING: v_9MUwNxAm5boRzDcr_62 has an image but not found in msb\n",
      "WARNING: v_3GGA5iF8CCq72n7s_35 has an image but not found in msb\n",
      "WARNING: v_nLELYmuh4laTLWqm_63 has an image but not found in msb\n",
      "WARNING: v_9DjBrCE8WdTEjrco_50 has an image but not found in msb\n",
      "WARNING: v_AHdEvc2HnCvReoU4_82 has an image but not found in msb\n",
      "WARNING: v_ZkwBlkHrwltjZvrA_72 has an image but not found in msb\n",
      "WARNING: v_R3vDd4VoEcCh9Hhx_63 has an image but not found in msb\n",
      "WARNING: v_NCbxrUwul1qcc0Sq_25 has an image but not found in msb\n",
      "WARNING: v_tM4PXWIhEZsw4VkU_4 has an image but not found in msb\n",
      "WARNING: v_DiqT1vd1iMccKreE_73 has an image but not found in msb\n",
      "WARNING: v_53qwul7eWkNDrRa2_9 has an image but not found in msb\n",
      "WARNING: v_hrOH5mbsJCXH76tc_73 has an image but not found in msb\n",
      "WARNING: v_wjbwYYBG2FY0uZ2w_23 has an image but not found in msb\n",
      "WARNING: v_opIERuxvLH9nFvtU_23 has an image but not found in msb\n",
      "WARNING: v_8koNP9fIA9Jyh8AG_6 has an image but not found in msb\n",
      "WARNING: v_9n80rXytWuZFb1Cj_14 has an image but not found in msb\n",
      "WARNING: v_TLiO4peqQl4YFvh5_88 has an image but not found in msb\n",
      "WARNING: v_A4qYbAWaxcJMFGNu_25 has an image but not found in msb\n",
      "WARNING: v_1hzeX7dftf49uLhx_97 has an image but not found in msb\n",
      "WARNING: v_tJYtDXbYustAGaRt_69 has an image but not found in msb\n",
      "WARNING: v_sLnzE6KUObep3zGD_14 has an image but not found in msb\n",
      "WARNING: v_Q6N6psUDbU6sSoBX_35 has an image but not found in msb\n",
      "WARNING: v_KfCCtSrE9vKABI9k_16 has an image but not found in msb\n",
      "WARNING: v_xKW0dcLgIWoxmY00_37 has an image but not found in msb\n",
      "WARNING: v_bz4xbdqNK46Syu8f_16 has an image but not found in msb\n",
      "WARNING: v_bOcUsJ5NUzTHDbkp_35 has an image but not found in msb\n",
      "WARNING: v_QyG9rhbXWMiAeDh8_17 has an image but not found in msb\n",
      "WARNING: v_JmzQznx75jb9iN03_18 has an image but not found in msb\n",
      "WARNING: v_yuXpFptTVfINE24z_38 has an image but not found in msb\n",
      "WARNING: v_AXVeVcujlx1GMmlP_26 has an image but not found in msb\n",
      "WARNING: v_YH0U8Yh2Lv9Lv82V_47 has an image but not found in msb\n",
      "WARNING: v_VoVWy3u7twAFS31i_36 has an image but not found in msb\n",
      "WARNING: v_eWdxwiGk5eIx7Qhw_119 has an image but not found in msb\n",
      "WARNING: v_8lLvOhR6KozZKku5_32 has an image but not found in msb\n",
      "WARNING: v_MkH3Gcu0nLKsLRT2_18 has an image but not found in msb\n",
      "WARNING: v_lNitLJjlyMBNHQIZ_15 has an image but not found in msb\n",
      "WARNING: v_r5fWbYX2tQCgp6FA_34 has an image but not found in msb\n",
      "WARNING: v_Ar89vOrFBMG6Mn9l_60 has an image but not found in msb\n",
      "WARNING: v_8DkfpRtcOiFELZAt_45 has an image but not found in msb\n",
      "WARNING: v_c6xC15YKcqJweWsW_6 has an image but not found in msb\n",
      "WARNING: v_WwZ8mvIKGKLdzmVV_77 has an image but not found in msb\n",
      "WARNING: v_tifD73I3jKem5y3a_46 has an image but not found in msb\n",
      "WARNING: v_z5n3ErbsqfRe0GYw_27 has an image but not found in msb\n",
      "WARNING: v_l9OYBGLQ0u37KvTc_35 has an image but not found in msb\n",
      "WARNING: v_p6CJeapbB92zVV1G_47 has an image but not found in msb\n",
      "WARNING: v_IIOPa0TIzN07onoR_34 has an image but not found in msb\n",
      "WARNING: v_ucH9K2JiRw3v5tTm_11 has an image but not found in msb\n",
      "WARNING: v_FdgzyRcevOTc7B64_35 has an image but not found in msb\n",
      "WARNING: v_C8de81U0FRCJZnvW_4 has an image but not found in msb\n",
      "WARNING: v_1oxeVPfCLD6euo0f_21 has an image but not found in msb\n",
      "WARNING: v_LfqByr6jRDEtUZLa_61 has an image but not found in msb\n",
      "WARNING: v_ODPESRxBt31kIYB2_8 has an image but not found in msb\n",
      "WARNING: v_VtkqqxVmzpEWPDTb_36 has an image but not found in msb\n",
      "WARNING: v_IiZfmS3ZDqDyVNuu_8 has an image but not found in msb\n",
      "WARNING: v_f97lzUm0wP7avNjO_5 has an image but not found in msb\n",
      "WARNING: v_QP60L4MatsY7GFcD_5 has an image but not found in msb\n",
      "WARNING: v_h8UUnqc8dpqurQ9a_50 has an image but not found in msb\n",
      "WARNING: v_2mg14iEOohJua5Ve_33 has an image but not found in msb\n",
      "WARNING: v_qT2VqygY8tVSwkSN_10 has an image but not found in msb\n",
      "WARNING: v_RHcAaIXpXb1bhncA_56 has an image but not found in msb\n",
      "WARNING: v_vM1yOGDGdbnpaZKj_55 has an image but not found in msb\n",
      "WARNING: v_w935QCAOzkICbd76_55 has an image but not found in msb\n",
      "WARNING: v_okFoLFm2X7P80w1X_29 has an image but not found in msb\n",
      "WARNING: v_fcjLDJCm1RjJ11yy_44 has an image but not found in msb\n",
      "WARNING: v_bmF497sOBzQdBRMQ_12 has an image but not found in msb\n",
      "WARNING: v_1G5P6VYdVK6fUlr0_27 has an image but not found in msb\n",
      "WARNING: v_FGOwB4Fai2sbA1U3_15 has an image but not found in msb\n",
      "WARNING: v_XBF0N74inhySkYtS_17 has an image but not found in msb\n",
      "WARNING: v_GP6JsiJwaNe3aS1Y_83 has an image but not found in msb\n",
      "WARNING: v_rMkuakoNJNfMeCeY_43 has an image but not found in msb\n",
      "WARNING: v_BtfIVLiLOIASdmxS_258 has an image but not found in msb\n",
      "WARNING: v_likb7WAmJPjrUtWk_20 has an image but not found in msb\n",
      "WARNING: v_VYUqAkx4lgBbiwDZ_133 has an image but not found in msb\n",
      "WARNING: v_zYR2aFv4gEBRjYdE_24 has an image but not found in msb\n",
      "WARNING: v_caRnsefFTgv7koPD_21 has an image but not found in msb\n",
      "WARNING: v_thAm590YBM9vmvFX_34 has an image but not found in msb\n",
      "WARNING: v_0CPLMMlBXoRUWCxP_5 has an image but not found in msb\n",
      "WARNING: v_ht3ETYGpBZjhSeRd_29 has an image but not found in msb\n",
      "WARNING: v_Ql8oxTtbxyKBmEz9_50 has an image but not found in msb\n",
      "WARNING: v_izJ1vTVzE2U37qoC_14 has an image but not found in msb\n",
      "WARNING: v_3BRFFSSilzon6Kjn_47 has an image but not found in msb\n",
      "WARNING: v_tlV58UvLeh7cqDUK_23 has an image but not found in msb\n",
      "WARNING: v_yLepwf5H7CamN3aR_68 has an image but not found in msb\n",
      "WARNING: v_KPOk47UNsPVDFLRE_47 has an image but not found in msb\n",
      "WARNING: v_NwoTlRtSYM5ZohPZ_30 has an image but not found in msb\n",
      "WARNING: v_1K9j6iQvUD6gZCdP_30 has an image but not found in msb\n",
      "WARNING: v_aozPIUHcf9UvKKnG_79 has an image but not found in msb\n",
      "WARNING: v_i0r9pEYdXOFK8788_55 has an image but not found in msb\n",
      "WARNING: v_ZstIAHxboNZ6RFjw_53 has an image but not found in msb\n",
      "WARNING: v_qVDFfAJDbWf8CGmG_9 has an image but not found in msb\n",
      "WARNING: v_H7cHizTVNxUVJr6b_42 has an image but not found in msb\n",
      "WARNING: v_jfGooLcsSgF3dP31_69 has an image but not found in msb\n",
      "WARNING: v_XRn3lBLIRdDMfckW_21 has an image but not found in msb\n",
      "WARNING: v_dczr4z1uiyJcCU7p_56 has an image but not found in msb\n",
      "WARNING: v_uS76S0u8F0jzeaYu_43 has an image but not found in msb\n",
      "WARNING: v_RgDf9T1sO6u99IVi_81 has an image but not found in msb\n",
      "WARNING: v_dlhJl07mfMDurzvw_15 has an image but not found in msb\n",
      "WARNING: v_RIWM1UZngDjBlPbC_40 has an image but not found in msb\n",
      "WARNING: v_07r92mDI9pqHDt3s_6 has an image but not found in msb\n",
      "WARNING: v_7OdrSuvlE4c8hfwM_49 has an image but not found in msb\n",
      "WARNING: v_GugX2RPKbKaWdYDh_32 has an image but not found in msb\n",
      "WARNING: v_NcL4SCqRHG25p3ec_6 has an image but not found in msb\n",
      "WARNING: v_h3qexmSoUf8smdHB_6 has an image but not found in msb\n",
      "WARNING: v_BbrZTQFVwuGBYSbJ_24 has an image but not found in msb\n",
      "WARNING: v_NAbcmdUpqFfGPgaB_33 has an image but not found in msb\n",
      "WARNING: v_54PHvVHZWxn5ryCV_58 has an image but not found in msb\n",
      "WARNING: v_2eMNf9vxisr3uNJu_3 has an image but not found in msb\n",
      "WARNING: v_JBLU1VMecn648jcj_13 has an image but not found in msb\n",
      "WARNING: v_QOBg7K0ZFQTI1qQr_15 has an image but not found in msb\n",
      "WARNING: v_bumkaeYZLOYjyRe1_50 has an image but not found in msb\n",
      "WARNING: v_092pvzWl8DSZUfoj_32 has an image but not found in msb\n",
      "WARNING: v_L0sQuukDxS5HNnhx_50 has an image but not found in msb\n",
      "WARNING: v_IixrFObs4McCAqOt_81 has an image but not found in msb\n",
      "WARNING: v_Bmu1TFkKzbApxV2w_64 has an image but not found in msb\n",
      "WARNING: v_r75eqFVmLdvpMUXx_32 has an image but not found in msb\n",
      "WARNING: v_PejyyBPr176V0AZR_21 has an image but not found in msb\n",
      "WARNING: v_aYpr2ZA8YYBxK0qq_111 has an image but not found in msb\n",
      "WARNING: v_ObjRqauAcSpatry3_112 has an image but not found in msb\n",
      "WARNING: v_hJGkrA88pYxxgJus_50 has an image but not found in msb\n",
      "WARNING: v_tIoHtJ76ma8EFptu_238 has an image but not found in msb\n",
      "WARNING: v_pQykDGb9GGRI4h5B_32 has an image but not found in msb\n",
      "WARNING: v_8a2SFGuNJ0RWlFId_51 has an image but not found in msb\n",
      "WARNING: v_FCVz4T0ambnJ1gIG_32 has an image but not found in msb\n",
      "WARNING: v_1KqMufrSYD29fdgF_82 has an image but not found in msb\n",
      "WARNING: v_91uTE2tcPQF5oDpL_33 has an image but not found in msb\n",
      "WARNING: v_pbR9AtoZdxVheZQr_101 has an image but not found in msb\n",
      "WARNING: v_zaqr5XhvMiFnvXA4_17 has an image but not found in msb\n",
      "WARNING: v_6KMPnVZwMKj5IHgU_37 has an image but not found in msb\n",
      "WARNING: v_RaHrCfkj8tF4RsAV_55 has an image but not found in msb\n",
      "WARNING: v_CJjArmGf0s0hyKLE_8 has an image but not found in msb\n",
      "WARNING: v_BzX3iptRdwDqB8oz_31 has an image but not found in msb\n",
      "WARNING: v_MthrRjgs72O8Pk1U_42 has an image but not found in msb\n",
      "WARNING: v_OntYfzQj4wPXprTM_37 has an image but not found in msb\n",
      "WARNING: v_Sb1pQAa2AETcHxBO_69 has an image but not found in msb\n",
      "WARNING: v_z7932hwdqYTD3NeZ_8 has an image but not found in msb\n",
      "WARNING: v_kTZ0uSxSsuLZgInJ_38 has an image but not found in msb\n",
      "WARNING: v_FGRapr12n8OO4UEi_12 has an image but not found in msb\n",
      "WARNING: v_brZ42rNNNE7bEQcJ_82 has an image but not found in msb\n",
      "WARNING: v_CYWm3fVuztXqGVfr_40 has an image but not found in msb\n",
      "WARNING: v_saFKgjK9kpkgOPD7_33 has an image but not found in msb\n",
      "WARNING: v_JSHxGD8FG1hrEsWb_23 has an image but not found in msb\n",
      "WARNING: v_XGaKPEz9S0eZcmFX_194 has an image but not found in msb\n",
      "WARNING: v_MStCs5Cl1nrMa5qs_50 has an image but not found in msb\n",
      "WARNING: v_ReTaLDS4OAQVZHaZ_36 has an image but not found in msb\n",
      "WARNING: v_2njyUVtDhL8CvlmD_41 has an image but not found in msb\n",
      "WARNING: v_CPc9La9vbwiZ4Msv_29 has an image but not found in msb\n",
      "WARNING: v_RYkxlDGtNrApgLGT_3 has an image but not found in msb\n",
      "WARNING: v_MFuV8bVU0ygLEm3I_42 has an image but not found in msb\n",
      "WARNING: v_tcl87m0ndeQz6VMu_21 has an image but not found in msb\n",
      "WARNING: v_svHNApZQTVIMYyCu_35 has an image but not found in msb\n",
      "WARNING: v_feMNliqMQdKQVIos_8 has an image but not found in msb\n",
      "WARNING: v_40Xcwgle9nlu9aEv_70 has an image but not found in msb\n",
      "WARNING: v_Z8Cjm31UnTDsMPw6_58 has an image but not found in msb\n",
      "WARNING: v_GRzL10q3lOLr7Rmj_45 has an image but not found in msb\n",
      "WARNING: v_N0PexsIW3vLoJkDY_43 has an image but not found in msb\n",
      "WARNING: v_7Zk8nKpZMSf7jUQW_33 has an image but not found in msb\n",
      "WARNING: v_t43pSuagPzkrV6Fy_58 has an image but not found in msb\n",
      "WARNING: v_S3PM7lwbSsvpLo23_82 has an image but not found in msb\n",
      "WARNING: v_s0Tg3zd6fTCrSqlI_63 has an image but not found in msb\n",
      "WARNING: v_V6l5G7PndmEfbPVn_24 has an image but not found in msb\n",
      "WARNING: v_uf8FrsUNfgRFtjqu_22 has an image but not found in msb\n",
      "WARNING: v_1M4Uutf9VM6Ndh0V_25 has an image but not found in msb\n",
      "WARNING: v_I4mB4uMJp5goZJ5c_44 has an image but not found in msb\n",
      "WARNING: v_HZxLNA2YMGk0eYBI_15 has an image but not found in msb\n",
      "WARNING: v_KcozDlLFBMhORAuV_45 has an image but not found in msb\n",
      "WARNING: v_M8PjQ36Cay9L8xDp_32 has an image but not found in msb\n",
      "WARNING: v_IW3pRWqD5CZD7rzn_40 has an image but not found in msb\n"
     ]
    }
   ],
   "source": [
    "kf_id_to_img_path = {}\n",
    "for i, item in enumerate(test_img_path):\n",
    "    kf_id = item.split('/')[-1].split('.')[0]\n",
    "    if kf_id in bad_to_good_kf_id:\n",
    "        kf_id_to_img_path[bad_to_good_kf_id[kf_id]] = item\n",
    "    else:\n",
    "        print(f'WARNING: {kf_id} has an image but not found in msb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../data/dryrun3_m18/kf_id2path.pkl', 'wb') as fout:\n",
    "    pickle.dump(kf_id_to_img_path, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "  # Get handles to input and output tensors\n",
    "  ops = tf.get_default_graph().get_operations()\n",
    "  all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "  tensor_dict = {}\n",
    "  for key in [\n",
    "      'num_detections', 'detection_boxes', 'detection_scores',\n",
    "      'detection_classes', 'detection_masks'\n",
    "  ]:\n",
    "    tensor_name = key + ':0'\n",
    "    if tensor_name in all_tensor_names:\n",
    "      tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "          tensor_name)\n",
    "  if 'detection_masks' in tensor_dict:\n",
    "    # The following processing is only for single image\n",
    "    detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "    detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "    real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "    detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "    detection_masks_reframed = tf.cast(\n",
    "        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "    # Follow the convention by adding back the batch dimension\n",
    "    tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "        detection_masks_reframed, 0)\n",
    "  image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = sess.run(tensor_dict,\n",
    "                         feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "  # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "  output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "  output_dict['detection_classes'] = output_dict[\n",
    "      'detection_classes'][0].astype(np.uint8)\n",
    "  output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "  output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "  if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf2/lib/python3.6/site-packages/ipykernel/__main__.py:10: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed out of 14607.\n",
      "100 images processed out of 14607.\n",
      "200 images processed out of 14607.\n",
      "300 images processed out of 14607.\n",
      "400 images processed out of 14607.\n",
      "500 images processed out of 14607.\n",
      "600 images processed out of 14607.\n",
      "700 images processed out of 14607.\n",
      "800 images processed out of 14607.\n",
      "900 images processed out of 14607.\n",
      "1000 images processed out of 14607.\n",
      "1100 images processed out of 14607.\n",
      "1200 images processed out of 14607.\n",
      "1300 images processed out of 14607.\n",
      "1400 images processed out of 14607.\n",
      "1500 images processed out of 14607.\n",
      "1600 images processed out of 14607.\n",
      "1700 images processed out of 14607.\n",
      "1800 images processed out of 14607.\n",
      "1900 images processed out of 14607.\n",
      "2000 images processed out of 14607.\n",
      "2100 images processed out of 14607.\n",
      "2200 images processed out of 14607.\n",
      "2300 images processed out of 14607.\n",
      "2400 images processed out of 14607.\n",
      "2500 images processed out of 14607.\n",
      "2600 images processed out of 14607.\n",
      "2700 images processed out of 14607.\n",
      "2800 images processed out of 14607.\n",
      "2900 images processed out of 14607.\n",
      "3000 images processed out of 14607.\n",
      "3100 images processed out of 14607.\n",
      "3200 images processed out of 14607.\n",
      "3300 images processed out of 14607.\n",
      "3400 images processed out of 14607.\n",
      "3500 images processed out of 14607.\n",
      "3600 images processed out of 14607.\n",
      "3700 images processed out of 14607.\n",
      "3800 images processed out of 14607.\n",
      "3900 images processed out of 14607.\n",
      "4000 images processed out of 14607.\n",
      "4100 images processed out of 14607.\n",
      "4200 images processed out of 14607.\n",
      "4300 images processed out of 14607.\n",
      "4400 images processed out of 14607.\n",
      "4500 images processed out of 14607.\n",
      "4600 images processed out of 14607.\n",
      "4700 images processed out of 14607.\n",
      "4800 images processed out of 14607.\n",
      "4900 images processed out of 14607.\n",
      "5000 images processed out of 14607.\n",
      "5100 images processed out of 14607.\n",
      "5200 images processed out of 14607.\n",
      "5300 images processed out of 14607.\n",
      "5400 images processed out of 14607.\n",
      "5500 images processed out of 14607.\n",
      "5600 images processed out of 14607.\n",
      "5700 images processed out of 14607.\n",
      "5800 images processed out of 14607.\n",
      "5900 images processed out of 14607.\n",
      "6000 images processed out of 14607.\n",
      "6100 images processed out of 14607.\n",
      "6200 images processed out of 14607.\n",
      "6300 images processed out of 14607.\n",
      "6400 images processed out of 14607.\n",
      "6500 images processed out of 14607.\n",
      "6600 images processed out of 14607.\n",
      "6700 images processed out of 14607.\n",
      "6800 images processed out of 14607.\n",
      "6900 images processed out of 14607.\n",
      "7000 images processed out of 14607.\n",
      "7100 images processed out of 14607.\n",
      "7200 images processed out of 14607.\n",
      "7300 images processed out of 14607.\n",
      "7400 images processed out of 14607.\n",
      "7500 images processed out of 14607.\n",
      "7600 images processed out of 14607.\n",
      "7700 images processed out of 14607.\n",
      "7800 images processed out of 14607.\n",
      "7900 images processed out of 14607.\n",
      "8000 images processed out of 14607.\n",
      "8100 images processed out of 14607.\n",
      "8200 images processed out of 14607.\n",
      "8300 images processed out of 14607.\n",
      "8400 images processed out of 14607.\n",
      "8500 images processed out of 14607.\n",
      "8600 images processed out of 14607.\n",
      "8700 images processed out of 14607.\n",
      "8800 images processed out of 14607.\n",
      "8900 images processed out of 14607.\n",
      "9000 images processed out of 14607.\n",
      "9100 images processed out of 14607.\n",
      "9200 images processed out of 14607.\n",
      "9300 images processed out of 14607.\n",
      "9400 images processed out of 14607.\n",
      "9500 images processed out of 14607.\n",
      "9600 images processed out of 14607.\n",
      "9700 images processed out of 14607.\n",
      "9800 images processed out of 14607.\n",
      "9900 images processed out of 14607.\n",
      "10000 images processed out of 14607.\n",
      "10100 images processed out of 14607.\n",
      "10200 images processed out of 14607.\n",
      "10300 images processed out of 14607.\n",
      "10400 images processed out of 14607.\n",
      "10500 images processed out of 14607.\n",
      "10600 images processed out of 14607.\n",
      "10700 images processed out of 14607.\n",
      "10800 images processed out of 14607.\n",
      "10900 images processed out of 14607.\n",
      "11000 images processed out of 14607.\n",
      "11100 images processed out of 14607.\n",
      "11200 images processed out of 14607.\n",
      "11300 images processed out of 14607.\n",
      "11400 images processed out of 14607.\n",
      "11500 images processed out of 14607.\n",
      "11600 images processed out of 14607.\n",
      "11700 images processed out of 14607.\n",
      "11800 images processed out of 14607.\n",
      "11900 images processed out of 14607.\n",
      "12000 images processed out of 14607.\n",
      "12100 images processed out of 14607.\n",
      "12200 images processed out of 14607.\n",
      "12300 images processed out of 14607.\n",
      "12400 images processed out of 14607.\n",
      "12500 images processed out of 14607.\n",
      "12600 images processed out of 14607.\n",
      "12700 images processed out of 14607.\n",
      "12800 images processed out of 14607.\n",
      "12900 images processed out of 14607.\n",
      "13000 images processed out of 14607.\n",
      "13100 images processed out of 14607.\n",
      "13200 images processed out of 14607.\n",
      "13300 images processed out of 14607.\n",
      "13400 images processed out of 14607.\n",
      "13500 images processed out of 14607.\n",
      "13600 images processed out of 14607.\n",
      "13700 images processed out of 14607.\n",
      "13800 images processed out of 14607.\n",
      "13900 images processed out of 14607.\n",
      "14000 images processed out of 14607.\n",
      "14100 images processed out of 14607.\n",
      "14200 images processed out of 14607.\n",
      "14300 images processed out of 14607.\n",
      "14400 images processed out of 14607.\n",
      "14500 images processed out of 14607.\n",
      "14600 images processed out of 14607.\n"
     ]
    }
   ],
   "source": [
    "det_results = {}\n",
    "image_shape = {}\n",
    "num_imgs = len(kf_id_to_img_path)\n",
    "for i, (imgid, imgpath) in enumerate(kf_id_to_img_path.items()):\n",
    "    try:\n",
    "        with open(imgpath, 'rb') as fin:\n",
    "            if imgpath.endswith('.ldcc'):\n",
    "                _ = fin.read(1024)\n",
    "            imgbin = fin.read()\n",
    "        imgbgr = cv2.imdecode(np.fromstring(imgbin, dtype='uint8'), cv2.IMREAD_COLOR)\n",
    "        image_np = imgbgr[:,:,[2,1,0]]\n",
    "        image_shape[imgid] = (image_np.shape[1], image_np.shape[0])\n",
    "    except Exception as ex:\n",
    "        print(imgid)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    \n",
    "    det_results[imgid] = output_dict\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} images processed out of {num_imgs}.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2mid = {}\n",
    "for item in label_map.item:\n",
    "    idx2mid[item.id] = item.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgid in det_results:\n",
    "    det_results[imgid]['detection_boxes'] = det_results[imgid]['detection_boxes'][:,[1,0,3,2]]\n",
    "    det_results[imgid]['detection_boxes_normalized'] = np.copy(det_results[imgid]['detection_boxes'])\n",
    "    det_results[imgid]['detection_boxes'] = (det_results[imgid]['detection_boxes'] * np.asarray(image_shape[imgid]*2)).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_results_2 = {}\n",
    "for imgid in det_results:\n",
    "    det_results_2[imgid] = []\n",
    "    for ii in range(det_results[imgid]['num_detections']):\n",
    "        if det_results[imgid]['detection_classes'][ii] == 0 or det_results[imgid]['detection_scores'][ii] == 0:\n",
    "            continue\n",
    "        det_results_2[imgid].append({\n",
    "            'label': idx2mid[det_results[imgid]['detection_classes'][ii]],\n",
    "            'score': det_results[imgid]['detection_scores'][ii],\n",
    "            'bbox': det_results[imgid]['detection_boxes'][ii],\n",
    "            'bbox_normalized': det_results[imgid]['detection_boxes_normalized'][ii],\n",
    "            'model': 'inception-resnet-faster-rcnn-oi'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/det_results_m18_dryrun3_kf_oi_1.pkl', 'wb') as fout:\n",
    "    pickle.dump(det_results_2, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../model_fusion/temp/imgsize_m18_dryrun3_kf.pkl', 'wb') as fout:\n",
    "    pickle.dump(image_shape, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../wsod/metadata/ont_m18/oi600_to_m18.pkl', 'rb') as fin:\n",
    "    label_map = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_results_filtered = {}\n",
    "for key, val in det_results_2.items():\n",
    "    det_results_filtered[key] = []\n",
    "    for det in val:\n",
    "        label = label_map.get(det['label'])\n",
    "        if label == None:\n",
    "            continue\n",
    "        det_results_filtered[key].append({\n",
    "            'label': label,\n",
    "            'score': det['score'],\n",
    "            'bbox': det['bbox'],\n",
    "            'bbox_normalized': det['bbox_normalized'],\n",
    "            'model': det['model'],            \n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/det_results_m18_dryrun3_kf_oi_1_filtered.pkl', 'wb') as fout:\n",
    "    pickle.dump(det_results_filtered, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Py3 w/ Tensorflow 2",
   "language": "python",
   "name": "py3tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
