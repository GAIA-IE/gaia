{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"../../lib\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../lib/object_detection/utils/visualization_utils.py:25: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-0f0a85121700>\", line 2, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n"
     ]
    }
   ],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = '../../checkpoints/faster_rcnn_nas_coco'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('../../lib/object_detection/data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_graph_def = tf.GraphDef()\n",
    "with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../data/background/raw/jpg.txt', 'r') as fin:\n",
    "    test_img_filenames = [line.strip() for line in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_filenames = ['../../../../data/background/raw/' + item for item in test_img_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image):\n",
    "  # Get handles to input and output tensors\n",
    "  ops = tf.get_default_graph().get_operations()\n",
    "  all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "  tensor_dict = {}\n",
    "  for key in [\n",
    "      'num_detections', 'detection_boxes', 'detection_scores',\n",
    "      'detection_classes', 'detection_masks'\n",
    "  ]:\n",
    "    tensor_name = key + ':0'\n",
    "    if tensor_name in all_tensor_names:\n",
    "      tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "          tensor_name)\n",
    "  if 'detection_masks' in tensor_dict:\n",
    "    # The following processing is only for single image\n",
    "    detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "    detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "    real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "    detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "    detection_masks_reframed = tf.cast(\n",
    "        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "    # Follow the convention by adding back the batch dimension\n",
    "    tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "        detection_masks_reframed, 0)\n",
    "  image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = sess.run(tensor_dict,\n",
    "                         feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "  # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "  output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "  output_dict['detection_classes'] = output_dict[\n",
    "      'detection_classes'][0].astype(np.uint8)\n",
    "  output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "  output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "  if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/home_at_filer2/tools/anaconda3/envs/py3tf/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed out of 13392.\n",
      "100 images processed out of 13392.\n",
      "200 images processed out of 13392.\n",
      "300 images processed out of 13392.\n",
      "400 images processed out of 13392.\n",
      "500 images processed out of 13392.\n",
      "600 images processed out of 13392.\n",
      "700 images processed out of 13392.\n",
      "800 images processed out of 13392.\n",
      "900 images processed out of 13392.\n",
      "1000 images processed out of 13392.\n",
      "1100 images processed out of 13392.\n",
      "1200 images processed out of 13392.\n",
      "1300 images processed out of 13392.\n",
      "1400 images processed out of 13392.\n",
      "1500 images processed out of 13392.\n",
      "1600 images processed out of 13392.\n",
      "1700 images processed out of 13392.\n",
      "1800 images processed out of 13392.\n",
      "1900 images processed out of 13392.\n",
      "2000 images processed out of 13392.\n",
      "2100 images processed out of 13392.\n",
      "2200 images processed out of 13392.\n",
      "2300 images processed out of 13392.\n",
      "2400 images processed out of 13392.\n",
      "2500 images processed out of 13392.\n",
      "2600 images processed out of 13392.\n",
      "2700 images processed out of 13392.\n",
      "2800 images processed out of 13392.\n",
      "2900 images processed out of 13392.\n",
      "3000 images processed out of 13392.\n",
      "3100 images processed out of 13392.\n",
      "3200 images processed out of 13392.\n",
      "3300 images processed out of 13392.\n",
      "3400 images processed out of 13392.\n",
      "3500 images processed out of 13392.\n",
      "3600 images processed out of 13392.\n",
      "3700 images processed out of 13392.\n",
      "3800 images processed out of 13392.\n",
      "3900 images processed out of 13392.\n",
      "4000 images processed out of 13392.\n",
      "4100 images processed out of 13392.\n",
      "4200 images processed out of 13392.\n",
      "4300 images processed out of 13392.\n",
      "4400 images processed out of 13392.\n",
      "4500 images processed out of 13392.\n",
      "4600 images processed out of 13392.\n",
      "4700 images processed out of 13392.\n",
      "4800 images processed out of 13392.\n",
      "4900 images processed out of 13392.\n",
      "5000 images processed out of 13392.\n",
      "5100 images processed out of 13392.\n",
      "5200 images processed out of 13392.\n",
      "5300 images processed out of 13392.\n",
      "5400 images processed out of 13392.\n",
      "5500 images processed out of 13392.\n",
      "5600 images processed out of 13392.\n",
      "5700 images processed out of 13392.\n",
      "5800 images processed out of 13392.\n",
      "5900 images processed out of 13392.\n",
      "6000 images processed out of 13392.\n",
      "6100 images processed out of 13392.\n",
      "../../../../data/background/raw/jpg/IC001E2X4.jpg.ldcc\n",
      "'NoneType' object is not subscriptable\n",
      "6200 images processed out of 13392.\n",
      "6300 images processed out of 13392.\n",
      "../../../../data/background/raw/jpg/IC001EHX8.jpg.ldcc\n",
      "'NoneType' object is not subscriptable\n",
      "../../../../data/background/raw/jpg/IC001EL09.jpg.ldcc\n",
      "'NoneType' object is not subscriptable\n",
      "6400 images processed out of 13392.\n",
      "6500 images processed out of 13392.\n",
      "6600 images processed out of 13392.\n",
      "6700 images processed out of 13392.\n",
      "6800 images processed out of 13392.\n",
      "6900 images processed out of 13392.\n",
      "7000 images processed out of 13392.\n",
      "7100 images processed out of 13392.\n",
      "7200 images processed out of 13392.\n",
      "7300 images processed out of 13392.\n",
      "7400 images processed out of 13392.\n",
      "7500 images processed out of 13392.\n",
      "7600 images processed out of 13392.\n",
      "7700 images processed out of 13392.\n",
      "7800 images processed out of 13392.\n",
      "7900 images processed out of 13392.\n",
      "8000 images processed out of 13392.\n",
      "8100 images processed out of 13392.\n",
      "8200 images processed out of 13392.\n",
      "8300 images processed out of 13392.\n",
      "8400 images processed out of 13392.\n",
      "8500 images processed out of 13392.\n",
      "8600 images processed out of 13392.\n",
      "8700 images processed out of 13392.\n",
      "8800 images processed out of 13392.\n",
      "8900 images processed out of 13392.\n",
      "9000 images processed out of 13392.\n",
      "9100 images processed out of 13392.\n",
      "9200 images processed out of 13392.\n",
      "9300 images processed out of 13392.\n",
      "9400 images processed out of 13392.\n",
      "9500 images processed out of 13392.\n",
      "9600 images processed out of 13392.\n",
      "9700 images processed out of 13392.\n",
      "9800 images processed out of 13392.\n",
      "9900 images processed out of 13392.\n",
      "10000 images processed out of 13392.\n",
      "10100 images processed out of 13392.\n",
      "10200 images processed out of 13392.\n",
      "10300 images processed out of 13392.\n",
      "10400 images processed out of 13392.\n",
      "10500 images processed out of 13392.\n",
      "10600 images processed out of 13392.\n",
      "10700 images processed out of 13392.\n",
      "10800 images processed out of 13392.\n",
      "10900 images processed out of 13392.\n",
      "11000 images processed out of 13392.\n",
      "11100 images processed out of 13392.\n",
      "11200 images processed out of 13392.\n",
      "11300 images processed out of 13392.\n",
      "11400 images processed out of 13392.\n",
      "11500 images processed out of 13392.\n",
      "11600 images processed out of 13392.\n",
      "11700 images processed out of 13392.\n",
      "11800 images processed out of 13392.\n",
      "11900 images processed out of 13392.\n",
      "12000 images processed out of 13392.\n",
      "12100 images processed out of 13392.\n",
      "12200 images processed out of 13392.\n",
      "12300 images processed out of 13392.\n",
      "12400 images processed out of 13392.\n",
      "12500 images processed out of 13392.\n",
      "12600 images processed out of 13392.\n",
      "12700 images processed out of 13392.\n",
      "12800 images processed out of 13392.\n",
      "12900 images processed out of 13392.\n",
      "13000 images processed out of 13392.\n",
      "13100 images processed out of 13392.\n",
      "13200 images processed out of 13392.\n",
      "13300 images processed out of 13392.\n"
     ]
    }
   ],
   "source": [
    "det_results = {}\n",
    "for i, filename in enumerate(test_img_filenames):\n",
    "    try:\n",
    "        with open(filename, 'rb') as fin:\n",
    "            _ = fin.read(1024)\n",
    "            imgbin = fin.read()\n",
    "        imgbgr = cv2.imdecode(np.fromstring(imgbin, dtype='uint8'), cv2.IMREAD_COLOR)\n",
    "        image_np = imgbgr[:,:,[2,1,0]]\n",
    "    except Exception as ex:\n",
    "        print(filename)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np)\n",
    "    \n",
    "    imgid = filename.split('/')[-1]\n",
    "    det_results[imgid] = output_dict\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} images processed out of {len(test_img_filenames)}.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/det_results_background_jpg_coco_1.pkl', 'wb') as fout:\n",
    "    pickle.dump(det_results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Py3 w/ Tensorflow",
   "language": "python",
   "name": "py3tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
