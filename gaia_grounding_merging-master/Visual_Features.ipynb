{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Point: Raw Data corpus_path change /dvmm-filer2/projects/AIDA/data/ldc_eval_m18/LDC2019E42_AIDA_Phase_1_Evaluation_Source_Data_V1.0/\n",
      "Check Point: RPI path change /home/bobby/aida_copy/AIDA/M18_copy/data/raw_files/RPI_TA1_E_asr/ltf_asr\n",
      "Check Point: Alireza path change: \n",
      " /home/bobby/aida_copy/AIDA/M18_copy/data/objdet_results/E/det_results_merged_34a_jpg.pkl \n",
      " /home/bobby/aida_copy/AIDA/M18_copy/data/objdet_results/E/det_results_merged_34b_kf.pkl \n",
      "\n",
      "CPU times: user 2.38 s, sys: 672 ms, total: 3.06 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import lmdb\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # GPU_ID\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True,allow_soft_placement=True)\n",
    "\n",
    "#############\n",
    "#ToDo: add capabality of finding jpgs in parent even if ID.ltf.xml or ID.psm.xml is given\n",
    "#############\n",
    "\n",
    "working_path = '/home/bobby/aida_copy/AIDA/M18_copy/data/'\n",
    "corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_eval_m18/LDC2019E42_AIDA_Phase_1_Evaluation_Source_Data_V1.0/'\n",
    "# corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun-updated_tmp/dryrun/'\n",
    "#corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun/' # D3\n",
    "#corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun/dryrun/' # D2\n",
    "\n",
    "img_path = corpus_path\n",
    "kfrm_path = corpus_path + 'data/video_shot_boundaries/representative_frames'\n",
    "parent_child_tab = corpus_path + 'docs/parent_children.sorted.tab'\n",
    "kfrm_msb = corpus_path + 'docs/masterShotBoundary.msb'\n",
    "print('Check Point: Raw Data corpus_path change',corpus_path)\n",
    "\n",
    "\n",
    "#rpi mention results in AIF\n",
    "p_f = 'E'\n",
    "video_asr_path = working_path + 'raw_files/RPI_TA1_'+p_f+'_asr/ltf_asr'\n",
    "video_map_path = working_path + 'raw_files/RPI_TA1_'+p_f+'_asr/map_asr'\n",
    "print('Check Point: RPI path change',video_asr_path)\n",
    "\n",
    "det_results_path_img = working_path + 'objdet_results/'+p_f+'/det_results_merged_34a_jpg.pkl'\n",
    "# det_results_path_img = working_path + 'objdet_results/det_results_merged_32_jpg.pkl'\n",
    "det_results_path_kfrm = working_path + 'objdet_results/'+p_f+'/det_results_merged_34b_kf.pkl'\n",
    "# det_results_path_kfrm = working_path + 'objdet_results/det_results_merged_33_kf.pkl'\n",
    "print('Check Point: Alireza path change:','\\n',det_results_path_img,'\\n', det_results_path_kfrm,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 11:42:54.809000 140317059749632 deprecation_wrapper.py:119] From /home/bobby/aida_copy/AIDA/M18_copy/code/utils.py:874: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading grounding pretrained model...\n",
      "Model Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 11:42:58.254809 140317059749632 deprecation_wrapper.py:119] From /home/bobby/aida_copy/AIDA/M18_copy/code/utils.py:875: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0705 11:42:58.282377 140317059749632 deprecation_wrapper.py:119] From /home/bobby/aida_copy/AIDA/M18_copy/code/utils.py:875: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "W0705 11:42:58.354227 140317059749632 deprecation_wrapper.py:119] From /home/bobby/aida_copy/AIDA/M18_copy/code/utils.py:877: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 11:43:04.331134 140317059749632 deprecation.py:323] From /dvmm-filer2/users/bobby/anaconda/envs/aida/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Loaded-----\n",
      "Loading done.\n",
      "CPU times: user 7.79 s, sys: 4.06 s, total: 11.9 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading grounding pretrained model\n",
    "print('Loading grounding pretrained model...')\n",
    "model_path = working_path+ 'models/model_ELMo_PNASNET_VOA_norm'\n",
    "sess, graph = load_model(model_path,config)\n",
    "input_img = graph.get_tensor_by_name(\"input_img:0\")\n",
    "mode = graph.get_tensor_by_name(\"mode:0\")\n",
    "v = graph.get_tensor_by_name(\"image_local_features:0\")\n",
    "v_bar = graph.get_tensor_by_name(\"image_global_features:0\")\n",
    "print('Loading done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-05 11:43:23.069239\n",
      "CPU times: user 1.03 s, sys: 224 ms, total: 1.25 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preparing dicts\n",
    "parent_dict, child_dict = create_dict(parent_child_tab)\n",
    "id2dir_dict_kfrm = create_dict_kfrm(kfrm_path, kfrm_msb, video_asr_path, video_map_path)\n",
    "#jpg\n",
    "path_dict = create_path_dict(os.path.join(corpus_path,'data/jpg/jpg/'))\n",
    "#mp4\n",
    "path_dict.update(create_path_dict_kfrm(id2dir_dict_kfrm))\n",
    "# print('HC000TJCP' in id2dir_dict_kfrm.keys())\n",
    "# print(id2dir_dict_kfrm.keys())\n",
    "\n",
    "#loading object detection results\n",
    "with open(det_results_path_img, 'rb') as f:\n",
    "    dict_obj_img = pickle.load(f)\n",
    "    \n",
    "with open(det_results_path_kfrm, 'rb') as f:\n",
    "    dict_obj_kfrm = pickle.load(f)\n",
    "print(datetime.now())\n",
    "# print(child_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 8 hours in total for Instance Features\n",
    "out_path_jpg = working_path + 'all_features/semantic_features_jpg.lmdb'\n",
    "out_path_kfrm = working_path + 'all_features/semantic_features_keyframe.lmdb'\n",
    "\n",
    "#opening lmdb environment\n",
    "lmdb_env_jpg = lmdb.open(out_path_jpg, map_size=int(1e11), lock=False)\n",
    "lmdb_env_kfrm = lmdb.open(out_path_kfrm, map_size=int(1e11), lock=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 18:29:40.096989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for image 4945 / 4947 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 19:54:51.626702\n",
      "CPU times: user 1d 9h 41s, sys: 2h 43min 43s, total: 1d 11h 44min 24s\n",
      "Wall time: 1h 25min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for image 4946 / 4947 \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#about 1.5 hour\n",
    "print(datetime.now())\n",
    "missed_children_jpg = []\n",
    "for i, key in enumerate(dict_obj_img):\n",
    "    imgs,_ = fetch_img(key+'.jpg.ldcc', parent_dict, child_dict, path_dict, level = 'Child')\n",
    "    if len(imgs)==0:\n",
    "        missed_children_jpg.append(key)\n",
    "        continue\n",
    "    img_batch, bb_ids, bboxes_norm = batch_of_bbox(imgs[0], dict_obj_img, key,\\\n",
    "                                        score_thr=0, filter_out=False)\n",
    "    if len(bb_ids)>0:\n",
    "        feed_dict = {input_img: img_batch, mode: 'test'}\n",
    "        v_pred = sess.run([v], feed_dict)[0]\n",
    "        for j,bb_id in enumerate(bb_ids):\n",
    "            mask = mask_fm_bbox(feature_map_size=(19,19),bbox_norm=bboxes_norm[j,:],order='xyxy')\n",
    "            if np.sum(mask)==0:\n",
    "                continue\n",
    "            img_vec = np.average(v_pred[j,:], weights = np.reshape(mask,[361]), axis=0)\n",
    "            save_key = key+'/'+str(bb_id)\n",
    "            with lmdb_env_jpg.begin(write=True) as lmdb_txn:\n",
    "                lmdb_txn.put(save_key.encode(), img_vec)\n",
    "    sys.stderr.write(\"Stored for image {} / {} \\r\".format(i, len(dict_obj_img)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 19:54:51.636527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for keyframe 21835 / 21837 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-01 02:08:46.869308\n",
      "CPU times: user 5d 15h 9min 38s, sys: 11h 49s, total: 6d 2h 10min 27s\n",
      "Wall time: 6h 13min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for keyframe 21836 / 21837 \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#about 4-6 hours\n",
    "print(datetime.now())\n",
    "missed_children_kfrm = []\n",
    "for i, key in enumerate(dict_obj_kfrm):\n",
    "    # key+'.mp4.ldcc'\n",
    "#     print('path from obj detecton for kfrm:',key+'.mp4.ldcc')\n",
    "    imgs,_ = fetch_img(key+'.mp4.ldcc', parent_dict, child_dict, path_dict, level = 'Child')\n",
    "    if len(imgs)==0:\n",
    "        missed_children_kfrm.append(key)\n",
    "        continue\n",
    "    img_batch, bb_ids, bboxes_norm = batch_of_bbox(imgs[0], dict_obj_kfrm, key,\\\n",
    "                                      score_thr=0, filter_out=False)\n",
    "    if len(bb_ids)>0:\n",
    "        feed_dict = {input_img: img_batch, mode: 'test'}\n",
    "        v_pred = sess.run([v], feed_dict)[0]\n",
    "        for j,bb_id in enumerate(bb_ids):\n",
    "            mask = mask_fm_bbox(feature_map_size=(19,19),bbox_norm=bboxes_norm[j,:],order='xyxy')\n",
    "            if np.sum(mask)==0:\n",
    "                continue\n",
    "            img_vec = np.average(v_pred[j,:], weights = np.reshape(mask,[361]), axis=0)\n",
    "            save_key = key+'/'+str(bb_id)\n",
    "            with lmdb_env_kfrm.begin(write=True) as lmdb_txn:\n",
    "                lmdb_txn.put(save_key.encode(), img_vec)\n",
    "    sys.stderr.write(\"Stored for keyframe {} / {} \\r\".format(i, len(dict_obj_kfrm)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missed_children_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missed_children_kfrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 3 hours in total for Instance Features\n",
    "out_path_jpg = working_path + 'all_features/instance_features_jpg.lmdb'\n",
    "out_path_kfrm = working_path + 'all_features/instance_features_keyframe.lmdb'\n",
    "\n",
    "\n",
    "#opening lmdb environment\n",
    "lmdb_env_jpg = lmdb.open(out_path_jpg, map_size=int(1e11), lock=False)\n",
    "lmdb_env_kfrm = lmdb.open(out_path_kfrm, map_size=int(1e11), lock=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading...\n",
      "pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvmm-filer2/users/bobby/anaconda/envs/aida/lib/python3.6/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Loaded-----\n",
      "CPU times: user 5.16 s, sys: 8.03 s, total: 13.2 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading instance matching pretrained model\n",
    "sess, graph = load_model(working_path+ 'models/model_universal_no_recons_ins_only',config)\n",
    "input_img = graph.get_tensor_by_name(\"input_img:0\")\n",
    "mode = graph.get_tensor_by_name(\"mode:0\")\n",
    "img_vec = graph.get_tensor_by_name(\"img_vec:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-05 12:04:15.054688\n",
      "2019-07-05 12:04:15.056789\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#about 0.5 hour\n",
    "print(datetime.now())\n",
    "missed_children_jpg = []\n",
    "for i, key in enumerate(dict_obj_img):\n",
    "    # Todo test\n",
    "    if 'HC0005KMS' not in key: #or 'HC0001H01' in key:\n",
    "        continue\n",
    "    print(i,key)\n",
    "\n",
    "    imgs,_ = fetch_img(key+'.jpg.ldcc', parent_dict, child_dict, path_dict, level = 'Child')\n",
    "    if len(imgs)==0:\n",
    "        missed_children_jpg.append(key)\n",
    "        continue\n",
    "    img_batch, bb_ids, bboxes_norm = batch_of_bbox(imgs[0], dict_obj_img, key,\\\n",
    "                                        score_thr=0, filter_out=False,img_size=(224,224))\n",
    "    \n",
    "    if len(bb_ids)>0:\n",
    "        # Test for Corpping bug\n",
    "        feed_dict = {input_img: img_batch, mode: 'test'}\n",
    "        img_vec_pred = sess.run([img_vec], feed_dict)[0]\n",
    "#     print('img_batch',img_batch)\n",
    "#         print('img_batch len:',len(img_batch),np.shape(img_batch))\n",
    "#         print('img_batch vec:',img_batch)\n",
    "#         print(np.shape(img_vec_pred))\n",
    "#         print('img_vec_pred',type(img_vec_pred),img_vec_pred)\n",
    "        for j,bb_id in enumerate(bb_ids):\n",
    "            save_key = key+'/'+str(bb_id)\n",
    "            with lmdb_env_jpg.begin(write=True) as lmdb_txn:\n",
    "                lmdb_txn.put(save_key.encode(), img_vec_pred[j,:])\n",
    "#                 print(sum(img_vec_pred[j,:]))\n",
    "    sys.stderr.write(\"Stored for image {} / {} \\r\".format(i, len(dict_obj_img)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for keyframe 21835 / 21837 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-30 17:58:58.874723\n",
      "CPU times: user 1d 4h 47min 57s, sys: 3h 8min 51s, total: 1d 7h 56min 48s\n",
      "Wall time: 1h 39min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stored for keyframe 21836 / 21837 \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#about 3 hours\n",
    "missed_children_kfrm = []\n",
    "for i, key in enumerate(dict_obj_kfrm):\n",
    "    imgs,_ = fetch_img(key+'.mp4.ldcc', parent_dict, child_dict, path_dict, level = 'Child')\n",
    "    if len(imgs)==0:\n",
    "        missed_children_kfrm.append(key)\n",
    "        continue\n",
    "    img_batch, bb_ids, bboxes_norm = batch_of_bbox(imgs[0], dict_obj_kfrm, key,\\\n",
    "                                      score_thr=0, filter_out=False,img_size=(224,224))\n",
    "    if len(bb_ids)>0:\n",
    "        feed_dict = {input_img: img_batch, mode: 'test'}\n",
    "        img_vec_pred = sess.run([img_vec], feed_dict)[0]\n",
    "        for j,bb_id in enumerate(bb_ids):\n",
    "            save_key = key+'/'+str(bb_id)\n",
    "            with lmdb_env_kfrm.begin(write=True) as lmdb_txn:\n",
    "                lmdb_txn.put(save_key.encode(), img_vec_pred[j,:])\n",
    "    sys.stderr.write(\"Stored for keyframe {} / {} \\r\".format(i, len(dict_obj_kfrm)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_children_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_children_kfrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
