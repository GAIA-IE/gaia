{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 1.01 s, total: 5.11 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rdflib import Graph, plugin, URIRef, Literal, BNode, RDF\n",
    "from rdflib.serializer import Serializer\n",
    "from rdflib.namespace import SKOS\n",
    "import rdflib\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import utils\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "\n",
    "# keep changing, cantact with Alireza\n",
    "sys.path.append(\"/home/bobby/aida_copy/AIDA/AIDA-Interchange-Format/python/\")\n",
    "import aida_interchange.aifutils as aifutils\n",
    "from aida_interchange.Bounding_Box import Bounding_Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport shutil,errno\\nsrc = '/dvmm-filer2/projects/AIDA/brian/...'\\ndst = '/home/bobby/aida_copy/AIDA/M18_copy/data/cu_ttl/_ttl_06/'\\n# def copyanything(src, dst):\\n#     try:\\n#         shutil.copytree(src, dst)\\n#     except OSError as exc: # python >2.5\\n#         if exc.errno == errno.ENOTDIR:\\n#             shutil.copy(src, dst)\\n#         else: raise\\nshutil.rmtree(dst)\\nshutil.copytree(src, dst)\\n# shutil.copyfile(src, dst)\\n\\n\\n# unzip\\nimport zipfile\\nfor zip_file in os.listdir(dst):\\n   zip_file = os.path.join(dst,zip_file)\\n   print(zip_file)\\n   zip_ref = zipfile.ZipFile(zip_file, 'r')\\n   zip_ref.extractall(dst)\\n   zip_ref.close()\\n# renaming \\nold_name, new_name = 'dry3', 'm18_dryrun_E2'\\nos.rename(os.path.join(dst, old_name), os.path.join(dst, new_name))\\nprint(old_name, '->', new_name)\\nold_name, new_name = 'dry3_i_c', 'm18_dryrun_im_E2'\\nos.rename(os.path.join(dst, old_name), os.path.join(dst, new_name))\\nprint(old_name, '->', new_name)\\n#\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " '''\n",
    "import shutil,errno\n",
    "src = '/dvmm-filer2/projects/AIDA/brian/...'\n",
    "dst = '/home/bobby/aida_copy/AIDA/M18_copy/data/cu_ttl/_ttl_06/'\n",
    "# def copyanything(src, dst):\n",
    "#     try:\n",
    "#         shutil.copytree(src, dst)\n",
    "#     except OSError as exc: # python >2.5\n",
    "#         if exc.errno == errno.ENOTDIR:\n",
    "#             shutil.copy(src, dst)\n",
    "#         else: raise\n",
    "shutil.rmtree(dst)\n",
    "shutil.copytree(src, dst)\n",
    "# shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "# unzip\n",
    "import zipfile\n",
    "for zip_file in os.listdir(dst):\n",
    "    zip_file = os.path.join(dst,zip_file)\n",
    "    print(zip_file)\n",
    "    zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
    "    zip_ref.extractall(dst)\n",
    "    zip_ref.close()\n",
    "# renaming \n",
    "old_name, new_name = 'dry3', 'm18_dryrun_E2'\n",
    "os.rename(os.path.join(dst, old_name), os.path.join(dst, new_name))\n",
    "print(old_name, '->', new_name)\n",
    "old_name, new_name = 'dry3_i_c', 'm18_dryrun_im_E2'\n",
    "os.rename(os.path.join(dst, old_name), os.path.join(dst, new_name))\n",
    "print(old_name, '->', new_name)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_dict(kv_dict, num = 2):\n",
    "    k_list = list(kv_dict.keys())[:num]\n",
    "    subdict =  {k: kv_dict[k] for k in k_list}\n",
    "    print('\\ndict len:', len(kv_dict))\n",
    "    return subdict\n",
    "\n",
    "def check_usc_grounding_dict(usc_grounding_dict, child_dict):\n",
    "    # test\n",
    "#     key = 'http://www.isi.edu/gaia/entities/b7ec53ef-3021-4eda-9429-7c87ccd1dc0d'\n",
    "#     if 'usc_vision' in usc_grounding_dict[key]['grounding'].values():\n",
    "#         print('find it')\n",
    "#     print('arrrr', usc_grounding_dict[key]['grounding']['system'])\n",
    "    \n",
    "    # Check Point: grounding_dict printing\n",
    "    top,i = 3,0\n",
    "    print('usc_grounded')\n",
    "    for en in usc_grounding_dict.keys():\n",
    "        for img_id in usc_grounding_dict[en]['grounding']:\n",
    "            if img_id  =='system':\n",
    "                continue\n",
    "            n_b = len(usc_grounding_dict[en]['grounding'][img_id]['bbox'])\n",
    "            if n_b>0:\n",
    "                img_dict = usc_grounding_dict[en]['grounding'][img_id]\n",
    "                if i < top: \n",
    "                    i+=1\n",
    "                    print(en)\n",
    "                    print(child_dict[img_id], img_id, usc_grounding_dict[en]['grounding'][img_id])\n",
    "                    print(sum(sum(img_dict['grounding_features'])),sum(sum(img_dict['instance_features'])))\n",
    "                    print('\\n')\n",
    "\n",
    "# cu_grndg_ent_pref = 'http://www.columbia.edu/AIDA/DVMM/Entities/GroundingBox/'+merge_version+'/'\n",
    "# cu_grndg_type_pref = 'http://www.columbia.edu/AIDA/DVMM/TypeAssertions/GroundingBox/'+merge_version+'/'\n",
    "# cu_grndg_clstr_img_pref = 'http://www.columbia.edu/AIDA/DVMM/Clusters/BoxOverlap/'+merge_version+'/'\n",
    "# cu_grndg_clstr_txt_pref = 'http://www.columbia.edu/AIDA/DVMM/Clusters/Grounding/'+merge_version+'/'\n",
    "        \n",
    "    \n",
    "def merge_usc(usc_dict_path, grounding_dict, child_dict):    \n",
    "    grounded = dict((k,v) for k,v in grounding_dict.items() if len(v['grounding']) > 0)\n",
    "    # print('grounded', top_dict(grounded),'\\n')\n",
    "    usc_grounding_dict = pickle.load(open(usc_dict_path,'rb'))\n",
    "    check_usc_grounding_dict(usc_grounding_dict, child_dict)\n",
    "    \n",
    "    print('intersection num', len(list(set(usc_grounding_dict.keys()).intersection(set(grounded.keys())))))\n",
    "    print('difference (num added from usc)', len(list(set(usc_grounding_dict.keys()).difference(set(grounded.keys())))))\n",
    "    conflict_set = list(set(usc_grounding_dict.keys()).intersection(set(grounded.keys())))\n",
    "    print('conflict entities:',conflict_set)\n",
    "    for k in conflict_set:\n",
    "        del usc_grounding_dict[k]\n",
    "\n",
    "    grounding_dict.update(usc_grounding_dict) # if key is same, the new pair will be appended  \n",
    "#     print('usc_grounding_dict', top_dict(usc_grounding_dict),'\\n')\n",
    "#     print('merged_grounding_dict', top_dict(merged_grounding_dict),'\\n')\n",
    "    print('usc_grounded_entity in merged_grounding_dict',grounding_dict[list(usc_grounding_dict.keys())[0]])\n",
    "    return grounding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-14 02:15:00.046268\n",
      "Check Point: ISI raw data change /dvmm-filer2/projects/AIDA/data/ldc_eval_m18/LDC2019E42_AIDA_Phase_1_Evaluation_Source_Data_V1.0/\n",
      "Check Point: version change /home/bobby/aida_copy/AIDA/M18_copy/data/all_features/grounding_dict_PT002_r2.pickle\n",
      "\n",
      "dict len: 42905\n",
      "grounding_dict {'http://www.isi.edu/gaia/entities/56ded7a2-9fcf-4875-92bd-0a70d7fa3d5a': {'textual_features': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'name': 'no_name', 'type_rdf': rdflib.term.URIRef('https://tac.nist.gov/tracks/SM-KBP/2019/ontologies/LDCOntology#PER'), 'mentions': {'f725a8fa797484b4383c2e6889ae06f63b393': {'grounding': {'no_image': {}}, 'textual_features': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'name': 'paratroopers', 'sentence': 'The ambush was initiated when DPR forces fired a rocket-propelled grenade at an armoured personnel carrier that had been carrying paratroopers causing the vehicle to explode'}}, 'source_type': 'JPG', 'language': 'English', 'grounding': {}}, 'http://www.isi.edu/gaia/entities/63b3a9fb-0c55-4ce3-bebc-26c6682667af': {'textual_features': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'name': 'no_name', 'type_rdf': rdflib.term.URIRef('https://tac.nist.gov/tracks/SM-KBP/2019/ontologies/LDCOntology#VEH.WheeledVehicle'), 'mentions': {'f725a8fa797484b4383c2e6889ae06f63b595': {'grounding': {'no_image': {}}, 'textual_features': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'name': 'vehicle', 'sentence': 'The ambush was initiated when DPR forces fired a rocket-propelled grenade at an armoured personnel carrier that had been carrying paratroopers causing the vehicle to explode'}}, 'source_type': 'JPG', 'language': 'English', 'grounding': {}}} \n",
      "\n",
      "Check Point: RPI path change /home/bobby/aida_copy/AIDA/M18_copy/data/rpi_ttl/RPI_TA1_PT002_r2\n",
      "Check Point: Brian cu_ttl path change /dvmm-filer2/projects/AIDA/brian/m18/m18_PT002_r2 /dvmm-filer2/projects/AIDA/brian/m18/m18_i_c_E2\n",
      "usc_grounded\n",
      "http://www.isi.edu/gaia/entities/f9ec19e5-7d74-4cef-8859-69c1197036b5\n",
      "['HC0000AKG', 'HC0002MV1', 'HC0002VSA'] HC0000AKJ.jpg.ldcc {'bbox': [array([ 26, 105, 692, 482], dtype=int32)], 'bbox_score': [0.9293742180000001], 'men-img-score': [0.9293742180000001], 'mention_phrase_used': 'represents the skills of this aircraft', 'grounding_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'instance_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'link_ids': [], 'link_scores': [], 'system': 'usc_vision', 'mention_src': 'aircraft'}\n",
      "0.0 0.0\n",
      "\n",
      "\n",
      "http://www.isi.edu/gaia/entities/61d984b5-3681-4239-a130-c5b29d508e0e\n",
      "['HC0000BVB'] HC0000BVD.jpg.ldcc {'bbox': [array([ 129,  105, 1195,  556], dtype=int32)], 'bbox_score': [0.7083594203], 'men-img-score': [0.7083594203], 'mention_phrase_used': 'port where visiting US Navy aircraft carriers will berth', 'grounding_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'instance_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'link_ids': [], 'link_scores': [], 'system': 'usc_vision', 'mention_src': 'aircraft'}\n",
      "0.0 0.0\n",
      "\n",
      "\n",
      "http://www.isi.edu/gaia/entities/61d984b5-3681-4239-a130-c5b29d508e0e\n",
      "['HC0000BVB'] HC0000BVF.jpg.ldcc {'bbox': [array([ 13,  18, 237, 140], dtype=int32)], 'bbox_score': [0.7911912203], 'men-img-score': [0.7911912203], 'mention_phrase_used': 'port where visiting US Navy aircraft carriers will berth', 'grounding_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'instance_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'link_ids': [], 'link_scores': [], 'system': 'usc_vision', 'mention_src': 'aircraft'}\n",
      "0.0 0.0\n",
      "\n",
      "\n",
      "intersection num 22\n",
      "difference (num added from usc) 9\n",
      "conflict entities: ['http://www.isi.edu/gaia/entities/72269e16-bbd7-4459-8ce6-a3015b7d6e8b', 'http://www.isi.edu/gaia/entities/9e03141d-ad25-4628-b78d-7507dc537d83', 'http://www.isi.edu/gaia/entities/21429d20-e71b-4053-9400-44937b75fd26', 'http://www.isi.edu/gaia/entities/f7b5557e-3173-498c-aeae-e421a6d8048a', 'http://www.isi.edu/gaia/entities/46d157ab-ddc5-4f32-8934-a049e05d1669', 'http://www.isi.edu/gaia/entities/369710ba-4d29-46bb-b241-e27f5673e290', 'http://www.isi.edu/gaia/entities/5af22ea7-7135-484b-b969-fcdbd12a011f', 'http://www.isi.edu/gaia/entities/6dfbb63c-d340-4719-ad86-f6c5480d6211', 'http://www.isi.edu/gaia/entities/85332057-203e-464d-bd9f-1d0ca06f958c', 'http://www.isi.edu/gaia/entities/52577248-a86f-49d8-a8c8-82c0dba28851', 'http://www.isi.edu/gaia/entities/61d984b5-3681-4239-a130-c5b29d508e0e', 'http://www.isi.edu/gaia/entities/bba379bb-bca4-4f83-824a-322f21a27185', 'http://www.isi.edu/gaia/entities/786ea61c-2dbb-487e-b57e-910b22f177bc', 'http://www.isi.edu/gaia/entities/4d207c26-00ce-47bd-ba70-7aa331fd5844', 'http://www.isi.edu/gaia/entities/f9ec19e5-7d74-4cef-8859-69c1197036b5', 'http://www.isi.edu/gaia/entities/e6131574-81d3-4bb1-a61e-c62933803246', 'http://www.isi.edu/gaia/entities/5dceb42b-fcdc-4e43-a7b9-7d52a2d8b0c6', 'http://www.isi.edu/gaia/entities/a3a33f7e-ac63-4c3a-bb0a-f7795b412426', 'http://www.isi.edu/gaia/entities/dd6f960c-d83b-4f6e-b83c-56648ac2433f', 'http://www.isi.edu/gaia/entities/06dee53b-2ec2-4d47-bddc-2ce4ea6370f7', 'http://www.isi.edu/gaia/entities/597bb1a6-acf3-4d7d-ae50-420baa66bfe5', 'http://www.isi.edu/gaia/entities/177a0a28-002b-4c03-b575-59d762474d27']\n",
      "usc_grounded_entity in merged_grounding_dict {'grounding': {'HC0000AKJ.jpg.ldcc': {'bbox': [array([ 26, 105, 692, 482], dtype=int32)], 'bbox_score': [0.9293742180000001], 'men-img-score': [0.9293742180000001], 'mention_phrase_used': 'which were built to the aircraft factory', 'grounding_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'instance_features': [array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], 'link_ids': [], 'link_scores': [], 'system': 'usc_vision', 'mention_src': 'aircraft'}, 'system': 'usc_vision'}}\n",
      "Check Point: USC gournding path change /home/bobby/aida_copy/AIDA/M18_copy/data/usc_dict/uscvision_grounding_output_cu_format_PT002_r2.pickle\n",
      "CPU times: user 2.73 s, sys: 2 s, total: 4.73 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it should returns \n",
    "print(datetime.now())\n",
    "working_path = '/home/bobby/aida_copy/AIDA/M18_copy/data/'\n",
    "corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_eval_m18/LDC2019E42_AIDA_Phase_1_Evaluation_Source_Data_V1.0/'\n",
    "# corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun-updated_tmp/dryrun/'\n",
    "#corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun3/dryrun/' # D3\n",
    "#corpus_path = '/dvmm-filer2/projects/AIDA/data/ldc_isi_dryrun/dryrun/' # D2\n",
    "img_path = corpus_path\n",
    "parent_child_tab = corpus_path + 'docs/parent_children.sorted.tab' # comes from others\n",
    "print('Check Point: ISI raw data change',corpus_path)\n",
    "\n",
    "p_f = 'PT002_r2' # E5\n",
    "p_f_share = 'E2'\n",
    "\n",
    "#grounding path\n",
    "grounding_dict_path = working_path + 'all_features/grounding_dict_'+p_f+'.pickle'\n",
    "\n",
    "parent_dict, child_dict = utils.create_dict(parent_child_tab)\n",
    "grounding_dict = pickle.load(open(grounding_dict_path,'rb'))\n",
    "print('Check Point: version change',grounding_dict_path)\n",
    "print('grounding_dict', top_dict(grounding_dict),'\\n')\n",
    "\n",
    "\n",
    "# Todo: change folder name\n",
    "rpi_ttl_path = working_path + 'rpi_ttl/RPI_TA1_'+p_f # 17th May\n",
    "# 'RPI_TA1_20190504' 'rpi_ttl/RPI_dryrun_'+p_f\n",
    "print('Check Point: RPI path change',rpi_ttl_path)\n",
    "\n",
    "# Todo: change path, version; two ttl folders from Brian\n",
    "cuttl_path = '/dvmm-filer2/projects/AIDA/brian/'\n",
    "update_version = 'm18/' #'dry_ttl_0625/' #'dry_ttl_0519/' # dry_ttl_0502/\n",
    "cu_ttl_path = cuttl_path + update_version + 'm18_'+p_f  #working_path + 'cu_ttl/'+update_version+'m18_'+p_f\n",
    "cu_ttl_ins_path = cuttl_path + update_version + 'm18_i_c_'+p_f_share #working_path + 'cu_ttl/'+update_version+'m18_i_c_'+p_f_share\n",
    "print('Check Point: Brian cu_ttl path change',cu_ttl_path,cu_ttl_ins_path)\n",
    "\n",
    "\n",
    "rpi_ttl_list = set([f.split('.')[0] for f in os.listdir(rpi_ttl_path)])\n",
    "cu_ttl_list = set([f.split('.')[0] for f in os.listdir(cu_ttl_path)])\n",
    "cu_ttl_ins_list = set([f.split('.')[0] for f in os.listdir(cu_ttl_ins_path)])\n",
    "\n",
    "cu_pref = 'http://www.columbia.edu/AIDA/DVMM/'\n",
    "cu_objdet_pref = 'http://www.columbia.edu/AIDA/DVMM/Entities/ObjectDetection/'\n",
    "\n",
    "# RUN00004 .. # May 8, 2019 for D2\n",
    "#'RUN00005' # May 19, 2019 for D2\n",
    "merge_version = 'RUN00006' # June 21, 2019 for D3\n",
    "cu_grndg_ent_pref = 'http://www.columbia.edu/AIDA/DVMM/Entities/GroundingBox/'+merge_version+'/'\n",
    "cu_grndg_type_pref = 'http://www.columbia.edu/AIDA/DVMM/TypeAssertions/GroundingBox/'+merge_version+'/'\n",
    "cu_grndg_clstr_img_pref = 'http://www.columbia.edu/AIDA/DVMM/Clusters/BoxOverlap/'+merge_version+'/'\n",
    "cu_grndg_clstr_txt_pref = 'http://www.columbia.edu/AIDA/DVMM/Clusters/Grounding/'+merge_version+'/'\n",
    "\n",
    "\n",
    "\n",
    "# USC path always changed \n",
    "usc_dict_path = working_path + 'usc_dict/'+ 'uscvision_grounding_output_cu_format_' + p_f + '.pickle' \n",
    "add_usc_result = True\n",
    "if add_usc_result:\n",
    "    grounding_dict = merge_usc(usc_dict_path, grounding_dict, child_dict)\n",
    "# Done: the [grounding_dict] will be updated by merged_grounding_dict\n",
    "usc_pref = 'http://www.usc.edu/AIDA/IRIS/'\n",
    "\n",
    "#'RUN00002' # May 19, 2019\n",
    "usc_merge_version = 'RUN00003' # June 22, 2019\n",
    "usc_grndg_ent_pref = 'http://www.usc.edu/AIDA/IRIS/Entities/GroundingBox/'+usc_merge_version+'/'\n",
    "usc_grndg_type_pref = 'http://www.usc.edu/AIDA/IRIS/TypeAssertions/GroundingBox/'+usc_merge_version+'/'\n",
    "usc_grndg_clstr_txt_pref = 'http://www.usc.edu/AIDA/IRIS/Clusters/Grounding/'+merge_version+'/'\n",
    "print('Check Point: USC gournding path change',usc_dict_path)\n",
    "\n",
    "\n",
    "\n",
    "rpi_entity_pref = 'http://www.isi.edu/gaia/entities/'\n",
    "gaia_prefix = 'http://www.isi.edu/gaia'\n",
    "# nist_ont_pref = '.../SM-KBP/2018/ontologies/InterchangeOntology#'\n",
    "nist_ont_pref = 'https://tac.nist.gov/tracks/SM-KBP/2019/ontologies/InterchangeOntology#'\n",
    "justified_by_ = URIRef(nist_ont_pref+'justifiedBy')\n",
    "entity_ = URIRef(nist_ont_pref+'Entity')\n",
    "sys_ = URIRef(nist_ont_pref+'system')\n",
    "hasName_ = URIRef(nist_ont_pref+'hasName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 ms, sys: 469 µs, total: 1.67 ms\n",
      "Wall time: 7.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###loading visual features\n",
    "#visual features path\n",
    "ins_img_path = working_path + 'all_features/instance_features_jpg.lmdb'\n",
    "ins_kfrm_path = working_path + 'all_features/instance_features_keyframe.lmdb'\n",
    "\n",
    "sem_img_path = working_path + 'all_features/semantic_features_jpg.lmdb'\n",
    "sem_kfrm_path = working_path + 'all_features/semantic_features_keyframe.lmdb'\n",
    "\n",
    "#load instance features\n",
    "ins_img_env = lmdb.open(ins_img_path, map_size=1e11, readonly=True, lock=False)\n",
    "ins_img_txn = ins_img_env.begin(write=False)\n",
    "ins_kfrm_env = lmdb.open(ins_kfrm_path, map_size=1e11, readonly=True, lock=False)\n",
    "ins_kfrm_txn = ins_kfrm_env.begin(write=False)\n",
    "\n",
    "#load semantic features\n",
    "sem_img_env = lmdb.open(sem_img_path, map_size=1e11, readonly=True, lock=False)\n",
    "sem_img_txn = sem_img_env.begin(write=False)\n",
    "sem_kfrm_env = lmdb.open(sem_kfrm_path, map_size=1e11, readonly=True, lock=False)\n",
    "sem_kfrm_txn = sem_kfrm_env.begin(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#undergoing functions\n",
    "def get_features(key,dtype,ftype):\n",
    "#     print(\"get_features for \",ftype)\n",
    "    if ftype=='instance':\n",
    "        if dtype=='jpg':\n",
    "            txn = ins_img_txn\n",
    "        elif dtype=='keyframe':\n",
    "            txn = ins_kfrm_txn\n",
    "        else:\n",
    "            return []\n",
    "    elif ftype=='semantic':\n",
    "        if dtype=='jpg':\n",
    "            txn = sem_img_txn\n",
    "        elif dtype=='keyframe':\n",
    "            txn = sem_kfrm_txn\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    value = txn.get(key.encode('utf-8'))\n",
    "    if value!=None:\n",
    "        return np.frombuffer(value, dtype='float32').tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-14 02:15:19.672818\n",
      "2019-07-14 02:37:37.532964\n",
      "CPU times: user 2.23 s, sys: 2.64 s, total: 4.87 s\n",
      "Wall time: 22min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# for Main Body\n",
    "import multiprocessing as mp\n",
    "# maybe changed when different runs\n",
    "print(datetime.now())\n",
    "export_dir = working_path + 'merged_ttl/merged_ttl_'+p_f\n",
    "#graph integration and modification\n",
    "#entity level\n",
    "\n",
    "def transferAIF(p_id):\n",
    "#for k,p_id in enumerate(parent_dict):\n",
    "\n",
    "    # Todo test\n",
    "#     if (k<8):\n",
    "#         continue\n",
    "#     print('k',k,p_id)\n",
    "    g = Graph()\n",
    "    \n",
    "    #load rpi graph if exists\n",
    "    if p_id in rpi_ttl_list:\n",
    "        turtle_path = os.path.join(rpi_ttl_path, p_id+'.ttl')\n",
    "        turtle_content = open(turtle_path).read()\n",
    "        g.parse(data=turtle_content, format='n3')\n",
    "    \n",
    "    #load and merge cu graph if exists\n",
    "    if p_id in cu_ttl_list:\n",
    "        turtle_path = os.path.join(cu_ttl_path, p_id+'.ttl')\n",
    "        turtle_content = open(turtle_path).read()\n",
    "        g.parse(data=turtle_content, format='n3')\n",
    "    \n",
    "    #load and merge cu graph for instance matching if exists\n",
    "    if p_id in cu_ttl_ins_list:\n",
    "        turtle_path = os.path.join(cu_ttl_ins_path, p_id+'.ttl')\n",
    "        turtle_content = open(turtle_path).read()\n",
    "        g.parse(data=turtle_content, format='n3')\n",
    "    \n",
    "    sys_instance_matching = aifutils.make_system_with_uri(g, cu_pref+'Systems/Instance-Matching/ResNet152')\n",
    "    sys_grounding = aifutils.make_system_with_uri(g, cu_pref+'Systems/Grounding/ELMo-PNASNET')\n",
    "    usc_sys_grounding = aifutils.make_system_with_uri(g, usc_pref + 'Systems/ZSGrounder')\n",
    "    \n",
    "    #find vision and text entities\n",
    "    sbj_all = set(g.subjects())\n",
    "    img_entities = {}\n",
    "    keyframe_entities = {}\n",
    "    ltf_entities = {}\n",
    "    for sbj in sbj_all:\n",
    "        sbj_name = sbj.toPython()\n",
    "        if cu_objdet_pref in sbj_name:\n",
    "            if sbj.__class__ == rdflib.term.URIRef:\n",
    "                if 'JPG' in sbj_name:\n",
    "                    img_id = '/'.join(sbj_name.split('/')[-2:])\n",
    "                    img_entities[img_id] = sbj\n",
    "                elif 'Keyframe' in sbj_name:\n",
    "                    kfrm_id = '/'.join(sbj_name.split('/')[-2:])\n",
    "                    keyframe_entities[kfrm_id] = sbj\n",
    "        elif rpi_entity_pref in sbj_name:\n",
    "            if sbj.__class__ == rdflib.term.URIRef and rpi_entity_pref in sbj_name:\n",
    "                ltf_entities[sbj_name] = sbj\n",
    "    \n",
    "    # Done\n",
    "#     if p_id in []:#['IC0011TIB']:\n",
    "#         continue\n",
    "#     print('k',k,p_id)\n",
    "#     if (g==None):\n",
    "#         print('p_id', k, p_id)\n",
    "    \n",
    "    ##adding private data to entities for cu grounding\n",
    "    #images\n",
    "    for key in img_entities:\n",
    "        dtype='jpg'\n",
    "        #instance features\n",
    "        ftype='instance'\n",
    "        data_instance = get_features(key,dtype,ftype)\n",
    "        \n",
    "        #semantic features\n",
    "        ftype='semantic'\n",
    "        data_semantic = get_features(key,dtype,ftype)\n",
    "        \n",
    "        #aggregation\n",
    "        j_d_i = json.dumps({'columbia_vector_instance_v1.0': data_instance})\n",
    "        j_d_s = json.dumps({'columbia_vector_grounding_v1.0': data_semantic})\n",
    "        entity = img_entities[key]\n",
    "        aifutils.mark_private_data(g, entity, j_d_i, sys_instance_matching)\n",
    "        aifutils.mark_private_data(g, entity, j_d_s, sys_grounding)\n",
    "    \n",
    "    #keyframes  \n",
    "    for key in keyframe_entities:\n",
    "        dtype='keyframe'\n",
    "        #instance features\n",
    "        ftype='instance'\n",
    "        data_instance = get_features(key,dtype,ftype)\n",
    "\n",
    "        #semantic features\n",
    "        ftype='semantic'\n",
    "        data_semantic = get_features(key,dtype,ftype)\n",
    "\n",
    "        #aggregation\n",
    "        j_d_i = json.dumps({'columbia_vector_instance_v1.0': data_instance})\n",
    "        j_d_s = json.dumps({'columbia_vector_grounding_v1.0': data_semantic})\n",
    "        entity = keyframe_entities[key]\n",
    "        aifutils.mark_private_data(g, entity, j_d_i, sys_instance_matching)\n",
    "        aifutils.mark_private_data(g, entity, j_d_s, sys_grounding)\n",
    "        \n",
    "        \n",
    "\n",
    "    cnt_img = {}\n",
    "    cnt_boxO = {}\n",
    "    cnt_ltf = {}\n",
    "    #add text features, grounding, linking\n",
    "    for key in ltf_entities:        \n",
    "        if key not in grounding_dict:\n",
    "            continue\n",
    "        entity_name = None\n",
    "        USC_GROUNDING = 'usc_vision' in grounding_dict[key]['grounding'].values()\n",
    "        if not USC_GROUNDING:\n",
    "#             print('our grounding')\n",
    "            #text features\n",
    "            j_d_t = json.dumps({'columbia_vector_text_v1.0': grounding_dict[key]['textual_features'].tolist()})\n",
    "            entity_ltf = ltf_entities[key]\n",
    "            aifutils.mark_private_data(g, entity_ltf, j_d_t, sys_grounding)\n",
    "        \n",
    "            #type and name of entity to be linked\n",
    "            type_rdf = grounding_dict[key]['type_rdf']\n",
    "            entity_name = grounding_dict[key]['name']\n",
    "            grndg_file_type = grounding_dict[key]['source_type']\n",
    "            \n",
    "        if entity_name is None:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        #keep track of entities with same names for avoiding clustering overlap\n",
    "        if entity_name in cnt_ltf:\n",
    "            cnt_ltf[entity_name] += 1\n",
    "        else:\n",
    "            cnt_ltf[entity_name] = 1\n",
    "        \n",
    "        clstr_prot_flag = False #cluster obj for entity_ltf not created yet\n",
    "        #adding grounding bboxes as new entities\n",
    "        for img_id in grounding_dict[key]['grounding']:\n",
    "            if img_id == 'system':\n",
    "                continue\n",
    "            grnd = grounding_dict[key]['grounding'][img_id]\n",
    "            for ii,bbox in enumerate(grnd['bbox']):\n",
    "                \n",
    "                if img_id in cnt_img: #to keep track of cnt of bbox of same image\n",
    "                    cnt_img[img_id] += 1\n",
    "                else:\n",
    "                    cnt_img[img_id] = 1\n",
    "                #add grounding bbox as entity\n",
    "                score = grnd['bbox_score'][ii]\n",
    "                if not USC_GROUNDING:\n",
    "                    type_eid = cu_grndg_type_pref+f\"{grndg_file_type}/{img_id.split('.')[0]}/{cnt_img[img_id]}/ERE\"\n",
    "                    ent_eid = cu_grndg_ent_pref+f\"{grndg_file_type}/{img_id.split('.')[0]}/{cnt_img[img_id]}\"\n",
    "                    entity_grnd = aifutils.make_entity(g, ent_eid, sys_grounding)\n",
    "                    type_assertion = aifutils.mark_type(g, type_eid, entity_grnd, type_rdf, sys_grounding, score)\n",
    "                elif USC_GROUNDING:\n",
    "                    type_eid = usc_grndg_type_pref+f\"{grndg_file_type}/{img_id.split('.')[0]}/{cnt_img[img_id]}/ERE\"\n",
    "                    ent_eid = usc_grndg_ent_pref+f\"{grndg_file_type}/{img_id.split('.')[0]}/{cnt_img[img_id]}\"\n",
    "                    entity_grnd = aifutils.make_entity(g, ent_eid, usc_sys_grounding)\n",
    "                    type_assertion = aifutils.mark_type(g, type_eid, entity_grnd, type_rdf, usc_sys_grounding, score)\n",
    "              \n",
    "                \n",
    "                # Done: \n",
    "                # 1. add if for the branches for image and keyframe.\n",
    "                # 2. add aifutils.mark_keyframe_video_justification\n",
    "                # 3. check output \n",
    "                # aifutils.mark_keyframe_video_justification(g, [entity, type_assertion], \"NYT_ENG_20181231_03\", \"keyframe ID\",\n",
    "                #                                                    bb2, system, 0.234)\n",
    "                # source: HC0005BR6_23\n",
    "                # print(img_id)\n",
    "\n",
    "                # Done: \n",
    "                # merge usc_grounding dict\n",
    "                # add usc_grounding entities and clusters\n",
    "                \n",
    "                # Test \n",
    "#                 print(\"type_assertion\",type_assertion, img_id)\n",
    "\n",
    "                \n",
    "                bb = Bounding_Box((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n",
    "                if not USC_GROUNDING:\n",
    "                    if 'JPG' in type_assertion:\n",
    "                        imgid = img_id.split('.')[0]\n",
    "                        justif = aifutils.mark_image_justification(g, [entity_grnd, type_assertion], imgid, bb, sys_grounding, score)\n",
    "#                     \n",
    "                    elif 'Keyframe' in type_assertion:\n",
    "                        imgid = img_id.split('.')[0].split('_')[0]\n",
    "                        kfid = img_id.split('.')[0].split('_')[1] # it should be keyframe image id or keyframe number\n",
    "                        justif = aifutils.mark_keyframe_video_justification(g, [entity, type_assertion], imgid, kfid, \\\n",
    "                                                                       bb, sys_grounding, score)\n",
    "                elif USC_GROUNDING:\n",
    "                    imgid = img_id.split('.')[0]\n",
    "                    justif = aifutils.mark_image_justification(g, [entity_grnd, type_assertion], imgid, bb, usc_sys_grounding, score)\n",
    "                else:\n",
    "                    print('[Merge Error] in Main Body: the type_assertion is wrong')\n",
    "                aifutils.add_source_document_to_justification(g, justif, p_id)\n",
    "                aifutils.mark_informative_justification(g, entity_grnd, justif)\n",
    "                \n",
    "            \n",
    "                if not USC_GROUNDING:\n",
    "                    grounding_features = grnd['grounding_features'][ii].tolist()\n",
    "                    instance_features = grnd['instance_features'][ii].tolist()\n",
    "                    #add private data to this very bbox entity\n",
    "                    j_d_g = json.dumps({'columbia_vector_grounding_v1.0': grounding_features})\n",
    "                    j_d_i = json.dumps({'columbia_vector_instance_v1.0': instance_features})\n",
    "                    aifutils.mark_private_data(g, entity_grnd, j_d_g, sys_grounding)\n",
    "                    aifutils.mark_private_data(g, entity_grnd, j_d_i, sys_instance_matching)\n",
    "\n",
    "                \n",
    "                #### add clusters\n",
    "                # Grounding Cluster\n",
    "                # Done: filtering about punctuation\n",
    "#                 translator = str.maketrans(string.punctuation, '_'*len(string.punctuation),'' )\n",
    "#                 'entity_name'.translate(translator)\n",
    "                entity_name_tmp = '_'.join(entity_name.split(' '))\n",
    "                entity_name_in_IRI = \"\".join(x if x.isalpha() or x.isdigit() or x =='_' else '-' for x in entity_name_tmp)\n",
    "                # '_'.join(entity_name.split(' '))\n",
    "                #gbbox entity to rpi entity\n",
    "                if not USC_GROUNDING:\n",
    "                    if not clstr_prot_flag: #create cluster if not present\n",
    "                        clst_eid = cu_grndg_clstr_txt_pref+f\"{entity_name_in_IRI}/{cnt_ltf[entity_name]}\"\n",
    "                        clusterObj = aifutils.make_cluster_with_prototype(g, clst_eid, entity_ltf, sys_grounding)\n",
    "                        clstr_prot_flag = True\n",
    "                    #cluster current bbox with current ltf_entity\n",
    "                    score = grnd['men-img-score'][ii]\n",
    "                    aifutils.mark_as_possible_cluster_member(g, entity_grnd, clusterObj, score, sys_grounding)\n",
    "                    # Done: add prototype as member\n",
    "                    aifutils.mark_as_possible_cluster_member(g, entity_ltf, clusterObj, 1, sys_grounding)\n",
    "                elif USC_GROUNDING:\n",
    "                    if not clstr_prot_flag: #create cluster if not present\n",
    "                        clst_eid = usc_grndg_clstr_txt_pref+f\"{entity_name_in_IRI}/{cnt_ltf[entity_name]}\"\n",
    "                        clusterObj = aifutils.make_cluster_with_prototype(g, clst_eid, entity_ltf, usc_sys_grounding)\n",
    "                        clstr_prot_flag = True\n",
    "                    #cluster current bbox with current ltf_entity\n",
    "                    score = grnd['men-img-score'][ii]\n",
    "                    aifutils.mark_as_possible_cluster_member(g, entity_grnd, clusterObj, score, usc_sys_grounding)\n",
    "                    # Done: add prototype as member\n",
    "                    aifutils.mark_as_possible_cluster_member(g, entity_ltf, clusterObj, 1, usc_sys_grounding)\n",
    "                    \n",
    "                # BoundingBox Overlap Cluster (Instance Matching)\n",
    "                #gbbox entity to objdet entity for instance matching\n",
    "                if not USC_GROUNDING:\n",
    "                    clstr_prot_b2b_flag = False\n",
    "                    for jj,img_id_link in enumerate(grnd['link_ids'][ii]): #for all objdet bboxes\n",
    "                        if img_id_link in img_entities:\n",
    "                            entity_link_img = img_entities[img_id_link]\n",
    "                        elif img_id_link in keyframe_entities:\n",
    "                            entity_link_img = keyframe_entities[img_id_link]\n",
    "                        else:\n",
    "                            continue\n",
    "                        if img_id in cnt_boxO: #to keep track of cnt of bbox overlap for same image\n",
    "                            cnt_boxO[img_id] += 1\n",
    "                        else:\n",
    "                            cnt_boxO[img_id] = 1\n",
    "                        if not clstr_prot_b2b_flag:\n",
    "                            clst_b2b_eid = cu_grndg_clstr_img_pref+f\"{img_id.split('.')[0]}/{cnt_boxO[img_id]}\"\n",
    "                            clusterObj_b2b = aifutils.make_cluster_with_prototype(g, clst_b2b_eid, entity_grnd, sys_grounding) # sys_instance_matching\n",
    "                            clstr_prot_b2b_flag = True\n",
    "\n",
    "                        score = grnd['link_scores'][ii][jj] #IoU of grnd bbox and objdet bbox\n",
    "                        aifutils.mark_as_possible_cluster_member(g, entity_link_img, clusterObj_b2b, score, sys_grounding) # sys_instance_matching\n",
    "                        # Done: add prototype as member\n",
    "                        aifutils.mark_as_possible_cluster_member(g, entity_grnd, clusterObj_b2b, 1, sys_grounding) # sys_instance_matching\n",
    "                    \n",
    "    # Check Point: merged_ttl_D2\n",
    "#     /data/bobby/AIDA/M18_copy/data/merged_ttl/merged_ttl_D2/\n",
    "#     IC0011VEA.ttl\n",
    "#     GroundingBox\n",
    "    with open(os.path.join(export_dir, p_id+'.ttl'), 'w') as fout:\n",
    "        serialization = BytesIO()\n",
    "        g.serialize(destination=serialization, format='turtle')\n",
    "        fout.write(serialization.getvalue().decode('utf-8'))\n",
    "    #sys.stdout.write('Key {}/{} \\r'.format(k,len(parent_dict)))                \n",
    "    sys.stdout.flush()\n",
    "pool = mp.Pool(processes=32)\n",
    "    #for x,y in candidateDic.items():\n",
    "    #print candidateDic.keys()\n",
    "res = pool.map(transferAIF, parent_dict.keys())\n",
    "\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBPedia to Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DB2link(graph):\n",
    "    '''\n",
    "    A function that gets graph and loads information in it.\n",
    "    '''\n",
    "    #get data and put in entity2mention dictionary\n",
    "    DB2en = {}\n",
    "    \n",
    "    entities = graph.subjects(predicate=RDF.type,object=entity_)\n",
    "    for entity in entities:\n",
    "        entity_id = entity.toPython()\n",
    "        link_nodes = list(g.objects(subject=entity,\n",
    "                                   predicate=URIRef(nist_ont_pref+'link')))\n",
    "        if len(link_nodes)==0:\n",
    "            continue\n",
    "        link_node = link_nodes[0]\n",
    "        link_string = list(g.objects(subject=link_node,\n",
    "                       predicate=URIRef(nist_ont_pref+'linkTarget')))[0].toPython()\n",
    "        \n",
    "        if link_string not in DB2en:\n",
    "            DB2en[link_string] = [entity]\n",
    "        else:\n",
    "            DB2en[link_string].append(entity)\n",
    "    return DB2en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#10 mins\n",
    "path_pref = '/home/bobby/aida_copy/AIDA/M18_copy/data/'\n",
    "\n",
    "RPI_AIF_path = rpi_ttl_path #path_pref + 'rpi_ttl/RPI_TA1_20190504'\n",
    "#'rpi_ttl/RPI_TA1_aif_dryrun20190410'#'rpi_ttl/RPI_TA1a_02102019_rerun_R4' #'rpi_ttl/RPI_dryrun_D1'\n",
    "# nist_ont_pref = '.../SM-KBP/2018/ontologies/InterchangeOntology#'\n",
    "nist_ont_pref = 'https://tac.nist.gov/tracks/SM-KBP/2019/ontologies/InterchangeOntology#'\n",
    "entity_ = URIRef(nist_ont_pref+'Entity')\n",
    "\n",
    "print('Creating id2link dictionary...')\n",
    "id2link = {}\n",
    "turtle_files = os.listdir(RPI_AIF_path)\n",
    "for i,file in enumerate(turtle_files):\n",
    "    if \".ttl\" not in file:\n",
    "        continue\n",
    "    turtle_path = os.path.join(RPI_AIF_path, file)\n",
    "    #loading turtle content\n",
    "    turtle_content = open(turtle_path).read()\n",
    "    g = Graph().parse(data=turtle_content, format='n3')\n",
    "    id_ = file.split('.')[0]\n",
    "    id2link[id_] = get_DB2link(g)\n",
    "    \n",
    "    sys.stdout.write('File {}/{} \\r'.format(i+1,len(turtle_files)))                \n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Todo: open to write\n",
    "with open(path_pref + 'tmp/id2link_dict_'+p_f+'.pickle', 'wb') as f:\n",
    "    pickle.dump(id2link,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Point: id2link printing\n",
    "print(top_dict(id2link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_private_data(g, img_entities, dtype):\n",
    "# #     print('add_private_data for instance and semantic:', dtype)\n",
    "#     for key in img_entities:\n",
    "#         #instance features\n",
    "#         ftype='instance'\n",
    "#         data_instance = get_features(key,dtype,ftype)\n",
    "        \n",
    "#         #semantic features\n",
    "#         ftype='semantic'\n",
    "#         data_semantic = get_features(key,dtype,ftype)\n",
    "        \n",
    "#         #aggregation\n",
    "#         j_d_i = json.dumps({'columbia_vector_instance_v1.0': data_instance})\n",
    "#         j_d_s = json.dumps({'columbia_vector_grounding_v1.0': data_semantic})\n",
    "#         entity = img_entities[key]\n",
    "#         aifutils.mark_private_data(g, entity, j_d_i, sys_instance_matching)\n",
    "#         aifutils.mark_private_data(g, entity, j_d_s, sys_grounding)\n",
    "\n",
    "#         return g\n",
    "\n",
    "    ##adding private data to entities\n",
    "    #images\n",
    "#     for key in img_entities:\n",
    "#         dtype='jpg'\n",
    "#         #instance features\n",
    "#         ftype='instance'\n",
    "#         data_instance = get_features(key,dtype,ftype)\n",
    "        \n",
    "#         #semantic features\n",
    "#         ftype='semantic'\n",
    "#         data_semantic = get_features(key,dtype,ftype)\n",
    "        \n",
    "#         #aggregation\n",
    "#         j_d_i = json.dumps({'columbia_vector_instance_v1.0': data_instance})\n",
    "#         j_d_s = json.dumps({'columbia_vector_grounding_v1.0': data_semantic})\n",
    "#         entity = img_entities[key]\n",
    "#         aifutils.mark_private_data(g, entity, j_d_i, sys_instance_matching)\n",
    "#         aifutils.mark_private_data(g, entity, j_d_s, sys_grounding)\n",
    "    \n",
    "    #keyframes  \n",
    "#     for key in keyframe_entities:\n",
    "#         dtype='keyframe'\n",
    "#         #instance features\n",
    "#         ftype='instance'\n",
    "#         data_instance = get_features(key,dtype,ftype)\n",
    "\n",
    "#         #semantic features\n",
    "#         ftype='semantic'\n",
    "#         data_semantic = get_features(key,dtype,ftype)\n",
    "\n",
    "#         #aggregation\n",
    "#         j_d_i = json.dumps({'columbia_vector_instance_v1.0': data_instance})\n",
    "#         j_d_s = json.dumps({'columbia_vector_grounding_v1.0': data_semantic})\n",
    "#         entity = keyframe_entities[key]\n",
    "#         aifutils.mark_private_data(g, entity, j_d_i, sys_instance_matching)\n",
    "#         aifutils.mark_private_data(g, entity, j_d_s, sys_grounding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rpi_en_path = path_pref + 'rpi_ttl/ttl_en_r0'\n",
    "rpi_en_list = set([f.split('.')[0] for f in os.listdir(rpi_en_path)])\n",
    "for k,p_id in enumerate(rpi_ttl_list):\n",
    "    g = Graph()\n",
    "    \n",
    "    #load rpi graph if exists\n",
    "    if p_id in rpi_en_list:\n",
    "        turtle_path = os.path.join(rpi_en_path, p_id+'.turtle')\n",
    "        turtle_content = open(turtle_path).read()\n",
    "        g.parse(data=turtle_content, format='n3')\n",
    "        entities = list(g.subjects(predicate=RDF.type,object=entity_))\n",
    "        for entity in entities:\n",
    "            if entity.toPython() in grounding_dict:\n",
    "                if len(grounding_dict[entity.toPython()]['grounding'])>0:\n",
    "                    for img_id in grounding_dict[entity.toPython()]['grounding']:\n",
    "                        if len(grounding_dict[entity.toPython()]['grounding'][img_id]['link_ids'])>0:\n",
    "                            print(p_id)\n",
    "    sys.stdout.write('Key {}/{} \\r'.format(k,len(rpi_en_list)))                \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_dict[entity.toPython()]['grounding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pref + 'tmp/entity2mention_dict_r0.pickle', 'rb') as f:\n",
    "    en2men = pickle.load(f)\n",
    "\n",
    "with open(path_pref + 'tmp/id2mentions_dict_r0.pickle', 'rb') as f:\n",
    "    id2men = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ttl = '/home/hassan/local/M9/rpi_ttl/ttl_en_r0/HC0000CVV.turtle'\n",
    "g = Graph()\n",
    "ttl_content = open(test_ttl).read()\n",
    "g.parse(data=ttl_content, format='n3')\n",
    "#json_string = g.serialize(format='json-ld', indent=4)\n",
    "#json_dict = json.loads(json_string)\n",
    "entities = list(g.subjects(predicate=RDF.type,object=entity_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = URIRef('http://www.isi.edu/gaia/entities/00668720-706b-415c-b76e-4b5419ef77ea')\n",
    "hasName_ = URIRef(nist_ont_pref+'hasName')\n",
    "entity_hasName = list(g.objects(predicate=hasName_,subject=entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2mention = utils.get_entity2mention(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2men['http://www.isi.edu/gaia/entities/00668720-706b-415c-b76e-4b5419ef77ea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_dict['http://www.isi.edu/gaia/entities/00668720-706b-415c-b76e-4b5419ef77ea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_ = URIRef(nist_ont_pref+'link')\n",
    "link_target_ = URIRef(nist_ont_pref+'linkTarget')\n",
    "for entity in entities:\n",
    "list(g.triples((None,link_,None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for just in just_by:\n",
    "    pv_data_rdf = list(g.objects(subject=just,\n",
    "                            predicate=URIRef(nist_ont_pref+'privateData')))[0]\n",
    "    dict_str=list(g.objects(subject=pv_data_rdf,\n",
    "                            predicate=URIRef(nist_ont_pref+'jsonContent')))[0].toPython()\n",
    "    print(dict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = utils.create_path_dict(os.path.join(corpus_path,'data/ltf/ltf/'))\n",
    "en2men = utils.get_entity2mention(g)\n",
    "filter_out=['pronominal_mention','GeopoliticalEntity','Organization','Location']\n",
    "id2men = utils.create_entity_dict(en2men, path_dict, caption_alignment_path=[], filter_out=filter_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(g.triples((None,None,None)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
